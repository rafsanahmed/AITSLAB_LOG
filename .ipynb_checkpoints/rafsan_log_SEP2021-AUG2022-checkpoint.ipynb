{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d05f6e8",
   "metadata": {},
   "source": [
    "# This log is the continuation of my second year of PhD (SEP2021-AUG2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456328f5",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### Active Projects\n",
    "\n",
    "01. NLP on biomedical text\n",
    "02. Swedish NLP on clinical data (future)\n",
    "03. Mariam and Malou cell profiler and unet works\n",
    "04. Ancient DNA (No news yet)\n",
    "05. Personal short project (need to find ideas)\n",
    "\n",
    "### General Targets\n",
    "\n",
    "01. NLP technical knowledge (papers, models, seminars)\n",
    "    a. BERT and BIOBERT\n",
    "    b. GPT3\n",
    "    c. SparkNLP\n",
    "    d. NER\n",
    "    e. RE\n",
    "    f. Neural coref\n",
    "    g. Topic modelling\n",
    "    h. Word2vec and basics\n",
    "    i. NLP stanford\n",
    "    \n",
    "02. ML/DL technical knowledge (papers, models, seminars)\n",
    "    a. TF\n",
    "    b. Pytorch\n",
    "    c. Mask-RCNN\n",
    "    d. Yolo\n",
    "    e. coco\n",
    "    f. efficientnet\n",
    "    g. DL michigan\n",
    "    h. Image segmentation pipelines\n",
    "    \n",
    "03. Finish all PhD courses including \"knowledge in collaboration\" in November\n",
    "04. Submit statII task\n",
    "05. Attend seminar from:\n",
    "    a. Swedish Bioinformatics\n",
    "    b. EDEON\n",
    "    c. Mobile heights Sweden\n",
    "    d. Peltarion\n",
    "    e. From copenhagen (Astrazeneca + Novonordisk + Jesper Johnsson + NLP)\n",
    "    f. LTH career fare from ARKAD\n",
    "    \n",
    "06. Hub AI leadership duties (speakers, events, talks, networking, management)\n",
    "07. Peer Learning Group events (Creation and management). Add to teaching duties.\n",
    "08. Grant writing (travel grant, short project)\n",
    "09. 1 new paper/2 weeks\n",
    "10. \n",
    "\n",
    "\n",
    "### Tools\n",
    "\n",
    "01. SQL\n",
    "02. Docker/singularity\n",
    "03. AWS/Azure\n",
    "04. Julia (trial)\n",
    "05. Agile\n",
    "06. C++ (if possible)\n",
    "\n",
    "_______________________________________________\n",
    "\n",
    "### Deadline by OCT 2021\n",
    "01. Create Docria collections for all texts in concern (json/pdf) [Done]\n",
    "02. Run models [Done for Biobert and biobert large]\n",
    "03. Find the best models, evaluate and test.\n",
    "04. Find short project idea and do some trials (sparkNLP?)\n",
    "05. ?\n",
    "\n",
    "### Deadline by NOV 2021\n",
    "01. Finalize short project and rough draft.\n",
    "02. ?\n",
    "\n",
    "### Deadline by DEC 2021\n",
    "01. Publishing 1 NLP paper\n",
    "02. Publish 1 paper based on the current results (NLP or otherwise)\n",
    "03. Draft 1 small project paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbcd90d",
   "metadata": {},
   "source": [
    "# 8th September 2021\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Convert docria collection to readable chunks\n",
    "\n",
    "### Description\n",
    "1. Get docria collection of full text\n",
    "2. Create a script to convert to sentence level.\n",
    "3. ?\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "\n",
    "\n",
    "\n",
    "### Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c593cd",
   "metadata": {},
   "source": [
    "# 10th September 2021\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Convert docria collection to readable chunks\n",
    "\n",
    "### Description\n",
    "1. Get docria collection of full text\n",
    "2. Create a script to convert to sentence level.\n",
    "3. ?\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "\n",
    "\n",
    "\n",
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22efdfcb",
   "metadata": {},
   "source": [
    "# 30th September 2021\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Minutes of the meeting with Sonja\n",
    "\n",
    "### Description\n",
    "#### Accomplished tasks:\n",
    "1. Docria full conversion of CORD 8th August,2021 text [should make a proper github repo for it]\n",
    "2. Ran 10 models biobert/biobert large\n",
    "3. Loaded pytorch models and got weird results\n",
    "\n",
    "#### Discussed future tasks\n",
    "1. What kind of error does the large models make? Make a script to compare individual sentences.\n",
    "2. Try to see which entities are missed and if it is the same word/words on multiple occasions.\n",
    "3. Something like \n",
    "| sentence | correct entity | missed entity |\n",
    "| ----- | ------- | ----- |\n",
    "| ----- | ------- | ----- |\n",
    "\n",
    "#### Poster and grants\n",
    "1. SBW deadline for poster and 3 minute video: 19th October\n",
    "2. Find grants for salary (if available)\n",
    "3. Mediatryck poster expenses: 400-600SEK according to website\n",
    "4. Find out the documents that are needed (ex: recommendation letter and contents)\n",
    "\n",
    "#### Poster content\n",
    "1. Background and intro\n",
    "2. Visuals: doc-collections -> NER -> CFC?\n",
    "3. Most important results\n",
    "4. Loaders and matching text in docria: text chopped up\n",
    "5. use gimp, pp, slideshare, biobert/hunflair poster\n",
    "\n",
    "#### Paper content\n",
    "1. Part I: Evaluation on HunFlair text\n",
    "2. Part II: Evaluation on CRAFT or another corpora\n",
    "3. Part III: Run on CORD - mayber paper II\n",
    "EG: Looking at CORD19, these are the most frequent cell lines\n",
    "4. \n",
    "\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "\n",
    "\n",
    "\n",
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bc37ac",
   "metadata": {},
   "source": [
    "# 19th October 2021\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Instructions \n",
    "\n",
    "### Description\n",
    "1. Find total number of entities discovered by each model\n",
    "2. list of entities sorted by mention frequency for each class, across the entire dataset.\n",
    "3. Number of entities per document\n",
    "\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "\n",
    "\n",
    "\n",
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff09c8",
   "metadata": {},
   "source": [
    "# 4th November 2021\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Data info\n",
    "\n",
    "### Description\n",
    "1. The data for biobert and Hunflair models was obtained from NER100 repo by Adam and Ola, curated by Marcus.\n",
    "The data is available on https://github.com/Aitslab/BioNLP/tree/master/Adam_Ola/ner_inputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "\n",
    "\n",
    "\n",
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104e04b0",
   "metadata": {},
   "source": [
    "# 11th November 2021\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Create NER for names\n",
    "\n",
    "### Description\n",
    "\n",
    "Looked at Regex patterns but did not manage to create a proper spacy phrasematcher NER model for names. Not sure how to approach this task.\n",
    "\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "\n",
    "\n",
    "\n",
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65217f65",
   "metadata": {},
   "source": [
    "# 4th February 2022\n",
    "---\n",
    "### Project\n",
    "GS paper\n",
    "\n",
    "### Aim\n",
    "Fix paper\n",
    "\n",
    "### Description\n",
    "\n",
    "1. Plurals for new file\n",
    "2. Add new terms to dictionary\n",
    "3. Finalize paper\n",
    "4. Check doaj and LU open access\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "\n",
    "\n",
    "\n",
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c568fc66",
   "metadata": {},
   "source": [
    "# 28th February 2022\n",
    "---\n",
    "### Project\n",
    "GS paper\n",
    "\n",
    "### Aim\n",
    "Fix paper\n",
    "\n",
    "### Description\n",
    "\n",
    "1. Plurals for new file\n",
    "diseases original: 12906 terms, merged and corrected: 13135 terms  + 7 added terms, 13124 after removing plurals \n",
    "\n",
    "[\"New coronavirus\",\n",
    "\"Novel coronavirus\",\n",
    "\"Chinese virus\",\n",
    "\"China virus\",\n",
    "\"Chinese coronavirus\",\n",
    "\"China coronavirus\",\n",
    "\"kung flu\"\n",
    "]\n",
    "\n",
    "virus original: 218 terms, merged and corrected: 357 terms, 352 after removing plurals\n",
    "\n",
    "2. Variants dictionary:\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "\n",
    "\n",
    "\n",
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d876fe7f",
   "metadata": {},
   "source": [
    "# 8th March 2022\n",
    "---\n",
    "### Project\n",
    "GS paper\n",
    "\n",
    "### Aim\n",
    "Finalize paper\n",
    "\n",
    "### Description\n",
    "\n",
    "1. Manually fixed XML gold standard annotations\n",
    "2. Created JSON from salma's script\n",
    "3. Created CSV from JSON (dict based)\n",
    "4. Uploaded manuscript_v2 folder to github\n",
    "5. Awaiting approval from Sonja\n",
    "\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "\n",
    "\n",
    "\n",
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a94746a",
   "metadata": {},
   "source": [
    "# 21st March 2022\n",
    "---\n",
    "### Project\n",
    "GS paper\n",
    "\n",
    "### Aim\n",
    "Finalize paper\n",
    "\n",
    "### Description\n",
    "\n",
    "Used the following functions to create the virus dictionary:\n",
    "1. dehyphenate and lowercase terms obtained from new literature\n",
    "2. add 2019 variants: 2019 as prefix and suffix plus 2019 new, 2019new, 2019 novel, 2019novel to the terms, given that 2019 or 19 doesn't exist within the term already\n",
    "3. interchange wuhan and hubei terms\n",
    "4. remove sentences with duplicate words as well as sentences with both new and novel terms together\n",
    "5. Merge with original file\n",
    "\n",
    "Used the following functions to create the disease dictionary:\n",
    "\n",
    "1. Load terms from new literature\n",
    "2. Load terms from cv file https://github.com/Aitslab/BioNLP/blob/master/sonja/cv_terms.md\n",
    "3. Load the new processed virus terms.\n",
    "4. Added new terms to (1) disease terms as well as well as the disease terms in (2).\n",
    "new_terms = ['new coronavirus','novel coronavirus','chinese virus','china virus','chinese coronavirus','china coronavirus','kung flu']\n",
    "5. Combine the new virus terms with terms from the cv file to make new disease terms. Place virus name, wuhan or hubei at [virus name or Wuhan or Hubei] position and virus name at [virus name] position to create variants. Remove hyphens, lowercase and remove duplicates.\n",
    "6. merge 4 and 5: disease terms and new disease terms from virus\n",
    "7. Interchange corona and coronavirus\n",
    "8. Interchange wuhan and hubei\n",
    "9. Interchange [disease, syndrome, infection, pneumonia and disorder]\n",
    "10. Remove duplicate words or when both new and novel exists in term\n",
    "11. Merge with original file\n",
    "\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "802 virus terms, 89893 disease terms\n",
    "\n",
    "\n",
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dd43ec",
   "metadata": {},
   "source": [
    "# 22nd-23rd March 2022\n",
    "---\n",
    "### Project\n",
    "GS paper\n",
    "\n",
    "### Aim\n",
    "Finalize paper\n",
    "\n",
    "### Description\n",
    "\n",
    "Added notebook to repo, fixed one comma\n",
    "Fixed jupyter notebook\n",
    "added reference\n",
    "merged scripts, added silver standard\n",
    "\n",
    "### Results/Conclusions\n",
    "\n",
    "Paper ready\n",
    "\n",
    "### Next Steps\n",
    "NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cdd74b",
   "metadata": {},
   "source": [
    "# 22nd-23rd March 2022\n",
    "---\n",
    "### Project\n",
    "GS paper\n",
    "\n",
    "### Aim\n",
    "Finalize paper\n",
    "\n",
    "### Description\n",
    "\n",
    "Added notebook to repo, fixed one comma\n",
    "Fixed jupyter notebook\n",
    "added reference\n",
    "merged scripts, added silver standard\n",
    "\n",
    "### Results/Conclusions\n",
    "\n",
    "Paper ready\n",
    "\n",
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600128f4",
   "metadata": {},
   "source": [
    "# 1st apr 2022\n",
    "---\n",
    "### Project\n",
    "NLP pipeline\n",
    "\n",
    "### Aim\n",
    "Running RE models by lykke/klara\n",
    "\n",
    "### Description\n",
    "\n",
    "#### tasks \n",
    "1. get artificial models from lykke klara\n",
    "2. put into lunarc\n",
    "3. Biobert issue for post processing\n",
    "4. Scibert integration\n",
    "5. lykke klara model should work on pipeline\n",
    "6. Evaluation script should work\n",
    "7. Is evaluation made on the custom label (corpora lykke axlin)\n",
    "8. Add micro f1 to eval script\n",
    "9. Run lykke eval script (important)\n",
    "\n",
    "#### Trials/comments on running their code\n",
    "1. Running eval standalone doesnt work, need metrics file. Loader has encoding issue, may need to fix it using utf8 encoding\n",
    "2. Running finetune alone requires imblearn module\n",
    "3. model doesn't load\n",
    "4. Issue with charmap (encoding) of the model. Need to explore alternatives\n",
    "\n",
    "### Results/Conclusions\n",
    "\n",
    "Both loading options from notebook and their script failed. Need to figure out a way to load\n",
    "\n",
    "### Next Steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca26f12b",
   "metadata": {},
   "source": [
    "# 4th apr 2022\n",
    "---\n",
    "### Project\n",
    "NLP pipeline\n",
    "\n",
    "### Aim\n",
    "Re evaluating biobert models\n",
    "\n",
    "### Description\n",
    "\n",
    "#### tasks \n",
    "1. Figure out why some large models have 0 F1 scores\n",
    "2. identify what max_seq_length within the tokenizer represents\n",
    "3. Run cell and gene models (also on training set) to see why they don't work\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "\n",
    "1. The large cell model for example didn't have any precision, recall score. Using the model on gene corpora and train set resulted in same results. \n",
    "\n",
    "** One important thing was that the cached files within the corpora had to be removed each time before training. Therefore that may have caused some issues with fine tuning?\n",
    "\n",
    "2. Max_seq_length within the tokenizer is set as a large number as default. However the model allows 512 tokens as a limit. Therefore, the tokenizer should also have 512 tokens as input and ignore the rest. The discrepency between the run model (with 192 cutoff) and the loaded, pretrained model (512) is still unclear\n",
    "\n",
    "3. Rinning cell model on gene, training set and test sets proved useless. The main issue seems to be with the model as there are no predictions\n",
    "\n",
    "### Next Steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba04ad",
   "metadata": {},
   "source": [
    "# 5th apr 2022\n",
    "---\n",
    "### Project\n",
    "NLP pipeline\n",
    "\n",
    "### Aim\n",
    "Re evaluating biobert models\n",
    "\n",
    "### Description\n",
    "\n",
    "#### tasks \n",
    "1. Trying to retrain cell models again\n",
    "2. Next is to try scibert from lykke/klara\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "\n",
    "\n",
    "\n",
    "### Next Steps\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
