{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is used to create a log of tasks for Rafsan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5th October, 2020\n",
    "---\n",
    "### Project\n",
    "Image Blur\n",
    "\n",
    "### Aim\n",
    "Read Jon's Masters thesis and create a plan for the project. Use available resources, such as the michigan tech DL lectures on youtube.\n",
    "\n",
    "### Description\n",
    "Gained some understanding into the project. The project revolves around classification between focused vs unfocused genome wide screen. Continuing lectures.\n",
    "\n",
    "### Next Steps\n",
    "Create a classifier for blurred images using the dataset from https://bbbc.broadinstitute.org/BBBC006\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6th October, 2020\n",
    "---\n",
    "### Project\n",
    "IMAGE Blur\n",
    "\n",
    "### Aim\n",
    "Start developing a model\n",
    "\n",
    "### Description\n",
    "Downloaded images from broad institute. Inspected them. Looked particularly on the planes 15,16 and 17 for focused images and 0,1 and 31 for unfocused images. Considering such planes are expected to give a proper classification among focused and blurred images.\n",
    "\n",
    "Looked at basic CNN structures\n",
    "\n",
    "### Next Steps\n",
    "Create a classifier for blurred images using the dataset from https://bbbc.broadinstitute.org/BBBC006\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13th October, 2020\n",
    "---\n",
    "### Project\n",
    "IMAGE Blur\n",
    "\n",
    "### Aim\n",
    "Finalize models\n",
    "\n",
    "### Description\n",
    "Created 3 different models. One with simple vgg block, one with three vgg blocks with dropout and on with vgg 16 transfer learning. The first ones gave 48-50% accuracy where transfer learning gave 72% accuracy\n",
    "\n",
    "### Next Steps\n",
    "Try to fine tune basee models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20th October, 2020\n",
    "---\n",
    "### Project\n",
    "IMAGE Blur\n",
    "\n",
    "### Aim\n",
    "Fine tune model\n",
    "\n",
    "### Description\n",
    "Try adam optimizer, adaptive learning rate. No improvement\n",
    "\n",
    "### Next Steps\n",
    "Try resnet and unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23th October, 2020\n",
    "---\n",
    "### Project\n",
    "NLP - pipeline\n",
    "\n",
    "### Aim\n",
    "Cover some basics about NLP\n",
    "\n",
    "### Description\n",
    "Go through terms like NER, tokenization, lemmatization etc\n",
    "\n",
    "### Next Steps\n",
    "Apply NER to models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28th October, 2020\n",
    "---\n",
    "### Project\n",
    "NLP - pipeline\n",
    "\n",
    "### Aim\n",
    "Attend NLP workshop\n",
    "\n",
    "### Description\n",
    "NLP workshop given by Johan frid. Was not that useful.\n",
    "\n",
    "### Next Steps\n",
    "Apply NER to models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd November 2020\n",
    "---\n",
    "### Project\n",
    "Image to Image translation\n",
    "\n",
    "### Aim\n",
    "UNET models\n",
    "\n",
    "### Description\n",
    "Look into how unet models work.\n",
    "\n",
    "### Next Steps\n",
    "Unet on LUNARC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4th November 2020\n",
    "---\n",
    "### Project\n",
    "Image to Image translation\n",
    "\n",
    "### Aim\n",
    "Malou's Unet code\n",
    "\n",
    "### Description\n",
    "Look into Malou's code and tried running. Pre-processing worked but training gave dependency issues.\n",
    "\n",
    "### Next Steps\n",
    "Fix issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5th November 2020\n",
    "---\n",
    "### Project\n",
    "Image to Image translation\n",
    "\n",
    "### Aim\n",
    "Malou's Unet code on LUNARC\n",
    "\n",
    "### Description\n",
    "Get LUNARC access. Tried running scripts there but have dependency issues.\n",
    "\n",
    "### Next Steps\n",
    "Fix issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9th November 2020\n",
    "---\n",
    "### Project\n",
    "Image to Image translation\n",
    "\n",
    "### Aim\n",
    "Setup kebnekaise and also use openvpn to access the server\n",
    "\n",
    "### Description\n",
    "Set up Kebnekaise using the following steps:\n",
    "\n",
    "1. Request a user account from SNIC -> HPC2N. The SNIC account was created before.\n",
    "2. Using the login information sent by Kebnekaise personnel (took 2 days), open kebnekaise server using SSH. Since I am using windows, putty was used to run terminal commands. A detailed description is provided in: https://www.hpc2n.umu.se/quickstart\n",
    "3. Once gained access, change the password.\n",
    "4. Now kebnekaise is accessible either through thinLinc or SSH. The server ID is: kebnekaise.hpc2n.umu.se. Note for Lunarc it is aurora.lunarc.lu.se\n",
    "\n",
    "### Results/Conclusion\n",
    "Have access to Kebnekaise now\n",
    "\n",
    "### Next Steps\n",
    "FIX openVPN issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12th November 2020\n",
    "---\n",
    "### Project\n",
    "Image to Image translation\n",
    "\n",
    "### Aim\n",
    "Work on pre-trined models on unet\n",
    "\n",
    "### Description\n",
    "Looked into inception v3 on unet. Aim to implement a simple unet model. Try running scripts on server.\n",
    "\n",
    "### Results/Conclusion\n",
    "Inception v3 appeared tricky to be run on Unet. I couldn't figure out how to use skip connections with pre-trained weights from Inception to Unet\n",
    "\n",
    "### Next Steps\n",
    "Read up on Unet and pre-trained models. Later, implement model with Malou's code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 13th November 2020\n",
    "---\n",
    "### Project\n",
    "Image to Image translation\n",
    "\n",
    "### Aim\n",
    "Work on pre-trined models on unet\n",
    "\n",
    "### Description\n",
    "Attempted running Kaggle Carvana colab notebook with VGG16. https://colab.research.google.com/github/MarkDaoust/models/blob/segmentation_blogpost/samples/outreach/blogs/segmentation_blogpost/image_segmentation.ipynb\n",
    "\n",
    "### Results/Conclusion\n",
    "Similar to inception, it was unsuccessfull. The colab notebook gave several errors with the image dataset. Need to build something from scratch\n",
    "\n",
    "### Next Steps\n",
    "Continue Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14th November 2020\n",
    "---\n",
    "### Project\n",
    "Image to Image translation\n",
    "\n",
    "### Aim\n",
    "Adapted Alexandro's tile script to generate 256x256 tiles as inputs for Malou's unet model.\n",
    "\n",
    "### Description\n",
    "Hardcoded the script to create 256x256 tiles. However, I detected several blank tiles when inspecting results.\n",
    "\n",
    "### Next Steps\n",
    "__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15th November 2020\n",
    "---\n",
    "### Project\n",
    "Image to Image translation\n",
    "\n",
    "### Aim\n",
    "Summerize challenge and visualize UNET architecture\n",
    "\n",
    "### Description\n",
    "Assisted the team with several small tasks including writing a script for the visualization of the Unet architecture.\n",
    "Source:https://machinelearningmastery.com/visualize-deep-learning-neural-network-model-keras/\n",
    "Script location: \"shared_data/malou_model5/viz/viz.py\" on Lunarc\n",
    "\n",
    "### Results/Conclusion\n",
    "It was an interesting challenge. However, more work needs to be done to implement pre-trained models on unet.\n",
    "\n",
    "### Next Steps\n",
    "Continue with the project for implementing pre-trained model with Unet (Autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17th November 2020\n",
    "---\n",
    "### Project\n",
    "NLP - Pipeline\n",
    "\n",
    "### Aim\n",
    "Run Antton's code for NER. Test out the following models:\n",
    "1. NER_biobert-BC5CDRChem_Antton_1, \n",
    "2. NER_biobert-JNLPBA_Antton_1, \n",
    "3. NER_biobert-NCBIdisease_Antton_1\n",
    "\n",
    "### Description\n",
    "Model files were not available. Asked Antton for the files.\n",
    "\n",
    "### Results/Conclusion\n",
    "Waiting for the files\n",
    "\n",
    "### Next Steps\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19th November 2020\n",
    "---\n",
    "### Project\n",
    "NLP - Pipeline\n",
    "\n",
    "### Aim\n",
    "Use Antton's models to evaluate.\n",
    "\n",
    "### Description\n",
    "Files were received from Antton. His models are located at his personal drive: https://drive.google.com/drive/u/2/folders/1vXBYTh7WmJh8bpmrYTevKLgDuMXemzTZ\n",
    "\n",
    "It was understood that Antton used BIOBERT code to evaluate his models. I will attempt to use the same code for evaluation.\n",
    "\n",
    "### Results/Conclusion\n",
    "Files were shared. However, downloading from Gdrive seemed to give only 1 GB zip files at a time. \n",
    "Trying to understand the evaluation code: \"utils/pubannotationevaluator.py\"\n",
    "\n",
    "### Next Steps\n",
    "Get evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23rd November 2020\n",
    "---\n",
    "### Project\n",
    "NLP - Pipeline\n",
    "\n",
    "### Aim\n",
    "Run Antton's Scripts and get results.\n",
    "\n",
    "### Description\n",
    "Was able to successfully run evaluation script, \"utils/use_evaluator.py\". This script is based on the original evaluation code \"utils/pubannotationevaluator.py\".\n",
    "\n",
    "### Results/Conclusions\n",
    "Running the script on \"GOLD william results\", gave a precision value of 26% and recall of 30%. I don't fully understand the script and the terms related to NLP yet.\n",
    "\n",
    "### Next Steps\n",
    "Do a literature review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24th November 2020\n",
    "---\n",
    "### Project\n",
    "NLP - Pipeline\n",
    "\n",
    "### Aim\n",
    "Improving background in NLP\n",
    "\n",
    "### Description\n",
    "Informed sonja about being unable to understand the script. She shared the following materials to go through.\n",
    "\n",
    "1. Transformers - Attention is all you need: https://arxiv.org/pdf/1706.03762.pdf\n",
    "2. BERT : https://arxiv.org/pdf/1810.04805.pdf\n",
    "3. BioBERT: https://academic.oup.com/bioinformatics/article/36/4/1234/5566506\n",
    "4. Blog Posts: https://jalammar.github.io/illustrated-transformer/\n",
    "5. Blog Posts: http://jalammar.github.io/illustrated-bert/\n",
    "6. CS224 NLP Stanford materials: https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z\n",
    "7. Pubannotation formats: http://www.pubannotation.org/docs/annotation-format/\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "Went through the BERT papers and CS224 lectures briefly. The lectures seemed a bit difficult to understand but the papers were nice.  \n",
    "\n",
    "### Next Steps\n",
    "Continue going through the materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27th November 2020\n",
    "---\n",
    "### Project\n",
    "NLP - Pipeline\n",
    "\n",
    "### Aim\n",
    "Check if the results in Antton's report matches mine\n",
    "\n",
    "### Description\n",
    "I am expected to run the following three models:\n",
    "1. NCBI Disease\n",
    "2. JNLPBA\n",
    "3. BC5CDR-Chem\n",
    "\n",
    "The firist two folders are located at the \"data/dataset/\" folder in Antton's repo.\n",
    "\n",
    "### Results/Conclusions\n",
    "Following the use_evaluater.py script from Antton, the precision-recall values matched table2 of Antton's preliminary report located in: https://github.com/Aitslab/BioNLP/blob/master/antton/Antton_ProjectReport.docx\n",
    "\n",
    "| Name         | Precision (%) | Recall (%) |\n",
    "|--------------|---------------|------------|\n",
    "| NCBI Disease | 31            | 35         |\n",
    "| JNLPBA       | 0             | 0          |\n",
    "\n",
    "BC5DR-CHEM was not available in the datasets.\n",
    "\n",
    "However, the results were comparable to table 2 of the report and table 1. The evaluation script seems to run on the dataset folder. The models shared by Antton also did not have the right format to run the evaluation script on.\n",
    "\n",
    "\n",
    "### Next Steps\n",
    "Need further understanding of the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30th November 2020\n",
    "---\n",
    "### Project\n",
    "NLP - Pipeline\n",
    "\n",
    "### Aim\n",
    "Meeting and discussion about Antton's models\n",
    "\n",
    "### Description\n",
    "The models were in drive. However while downloading, the models split into multiple files. We needed to contact Antton about the files.\n",
    "\n",
    "### Results/Conclusions\n",
    "On 2nd December it was decided to individually download the files despite the split\n",
    "### Next Steps\n",
    "Continuation with the NLP course (Udemy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st and 2nd December 2020\n",
    "---\n",
    "Setting up the Laptop including Anaconda, Thinlinc and additional programs for servers and setting up CUDA. CUDA setup was not completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd December 2020\n",
    "---\n",
    "### Project\n",
    "NLP - Pipeline\n",
    "\n",
    "### Aim\n",
    "Download Models run by Antton. Add to Lunarc.\n",
    "\n",
    "### Description\n",
    "The models are 6-8 GB folders each. I have initially tried to download the three models described in Antton's paper.\n",
    "1. NCBI Disease\n",
    "2. JNLPBA\n",
    "3. BC5CDR-Chem\n",
    "\n",
    "### Results/Conclusions\n",
    "Number 2 and 3 has already been added to Lunarc at snic2020-6-41/rafsan/Antton models. The rest of the models follow\n",
    "\n",
    "### Next Steps\n",
    "Continuation with the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd December 2020\n",
    "---\n",
    "### Project\n",
    "Citizen Scientist game by Salma\n",
    "\n",
    "### Aim\n",
    "Run the game and annotate 100 images\n",
    "\n",
    "### Description\n",
    "The citizen scientist game developed by Salma has 4 levels with unique tasks. The idea is to annotate 100 images and do a test for the game itself to see that everything is working smoothly.\n",
    "\n",
    "The game was run by using bash and separately using Anaconda. In both instances we received multiple bugs, mostly revolving around local variables (for example \"con\" and \"P1 full filename\") referenced before assignment.\n",
    "\n",
    "### Results/Conclusions\n",
    "The game needs debugging. I am not sure if the errors occur due to LUNARC or the code itself. I have forwarded my findings to Salma.\n",
    "\n",
    "### Next Steps\n",
    "Continue with annotations after receiving the debugged game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4th December 2020\n",
    "---\n",
    "### Project\n",
    "Citizen Scientist game by Salma\n",
    "\n",
    "### Aim\n",
    "Get the game running on my lunarc\n",
    "\n",
    "### Description\n",
    "Had several discussions with Salma about the bugs in the game. At first I tried to create a new environment similar to her local system. However that did not pan out. Later we discovered that, these specific errors can arise when trying to run the game from the folder \"Rafsan Game\" and not the game folder within this folder. The environment loaded but the specific file paths were one level up. I ran the game in the correct folder and it executed smoothly.\n",
    "\n",
    "### Results/Conclusions\n",
    "The game works on my lunarc now.\n",
    "\n",
    "### Next Steps\n",
    "Salma asked me to annotate 100 images for each channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7th December 2020\n",
    "---\n",
    "### Project\n",
    "Citizen Scientist game by Salma\n",
    "\n",
    "### Aim\n",
    "Annotation of 100 images for each level\n",
    "\n",
    "### Description\n",
    "Annotated images for level 1,2,3 and 4. Level 1: blur detection seemed ambiguous.\n",
    "\n",
    "### Results/Conclusions\n",
    "Annotations are done.\n",
    "\n",
    "### Next Steps\n",
    "Need to discuss the results with Salma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8-11th December 2020\n",
    "---\n",
    "Continuation of the NLP course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21 December 2020 onward\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Rerun the scripts from petter alexander github repo at: https://github.com/askft/thesis-code\n",
    "\n",
    "### Description\n",
    "1. Cloned the repo to local. Created the anaconda environment py37\n",
    "\n",
    "\n",
    "2. Received an error on installing requirements from the repo. Commented out the pubmed_parser line within the requirements.txt file. Installation from cmd line doesn't work. The downloader.py requires pubmed_parser. Therefore, I installed it to the conda env by directly using the command: pip install git+git://github.com/titipata/pubmed_parser.git\n",
    "\n",
    "    SKlearn is not in the requirements and not installed. This should be added to the requirements.\n",
    "\n",
    "\n",
    "3. For running the code, there are several steps. However I could reach only the following step before running into an issue:\n",
    "\n",
    "  a. For step 1 (in the repo), the main.py script needs to run. But in order to do that, the config.json file needs to be set.\n",
    "  For this step, both cord_loader and downloader in the pipeline gave an error. However, it seems that the cord loader is only lacking the metadata.csv file whereas the downloader has an unicode error. \" character '\\u03b2'\" which refers to a beta.\n",
    "  \n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "The downloader doesn't function well. Need to examine this more. \n",
    "\n",
    "### Next Steps\n",
    "Continuations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4th January 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Rerun the scripts from petter alexander github repo at: https://github.com/askft/thesis-code\n",
    "\n",
    "### Description\n",
    "\n",
    "The cord-19 data was downloaded from the source: https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html. \n",
    "\n",
    "The file was extracted into a newly created data/cord folder. The metadata.csv file was placed in this folder. The config.json file was changed to include the cord loader instead by changing the boolean fields to true/false. Running the main.py code gave the same **\"UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 1507: character maps to undefined> \"**. It seems that this error may be caused due to encoding when loading the file. A stackoverflow search identifies the issue to be a UTF-8 encoding error. The following adjustment may be needed somewhere in the loader script:\\\n",
    "\n",
    "```\n",
    "file = open(filename, encoding=\"utf8\")\n",
    "```\n",
    "  \n",
    "I tried to make the above change to line 23 of the cord_loader script in the scipt/ folder. But that did not change the outcome.\n",
    "  \n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "Contacted Petter-Alexander on discord. Waiting for their reply.\n",
    "\n",
    "### Next Steps\n",
    "Continuation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14th January 2020\n",
    "---\n",
    "### Event\n",
    "Meeting with Sonja\n",
    "\n",
    "### Points\n",
    "1. Run petter-alexander code by implementing changes\n",
    "2. Prepare a jupyter-notebook like Antton's at https://github.com/Aitslab/BioNLP/blob/master/evaluation/antton/evaluations_table.csv\n",
    "3. Look for grants at the intramed database:  \n",
    "    a. https://www.med.lu.se/english/intramed  \n",
    "    b. https://www.med.lu.se/intramed/stipendier_anslag_fraan_fakulteten  \n",
    "    c. https://www.keystonesymposia.org/ks/online  \n",
    "    d. https://www.grc.org/  \n",
    "    e. https://www.ebi.ac.uk/  \n",
    "    f. https://meetings.cshl.edu/meetingshome.aspx  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18th January 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Rerun the scripts from petter alexander github repo at: https://github.com/askft/thesis-code\n",
    "\n",
    "### Description\n",
    "Salma was able to run the scripts in lunarc while I wasn't able to run on my computer. I eventually proceeded to install ubuntu for windows in my computer and run the scripts on the windows subsystems for linux. There is a nice tutorial for this at: https://www.youtube.com/watch?v=X-DHaQLrBi8  \n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "On this subsystem both downloader and cord_loader worked. However the sentencer and NER did not work. \n",
    "\n",
    "### Next Steps\n",
    "Continuation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20th January 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Rerun the scripts from petter alexander github repo at: https://github.com/askft/thesis-code\n",
    "\n",
    "### Description\n",
    "With Salma's help, I was able to run CORD loader and get metadata.json in the same folder with metadata.csv. This works with ubuntu\n",
    "\n",
    "I proceeded with the sentencer script and that executed on ubuntu as well.  \n",
    "\n",
    "Now, the NER script did not execute since there was no BERT tokenizer. The authors did not mention this on their repo.\n",
    "\n",
    "In the repo there is a mention of  converting biobert to ONNX. But nothing specific about biobert installation etc. After downloading the model I was able to run the NER script. Although it shows that the NER is limited to 15 articles.\n",
    "\n",
    "The re script may have an incorrect path. data/cord/... instead of data/...\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "Sent petter alexander an email with my comments. \n",
    "\n",
    "### Next Steps\n",
    "Continuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5th February 2020\n",
    "---\n",
    "### Meeting\n",
    "Half yearly assessment\n",
    "\n",
    "### Aim\n",
    "Review performance and plan ahead\n",
    "\n",
    "### Description\n",
    "Following issues were discussed:\n",
    "\n",
    "* Improve productivity by working in blocks and working 8 hours\n",
    "* Work fika (except mon and wed)\n",
    "* Network with people from the industry. Attent career fare from MED + SCI +  LTH and ask career people for including me in newsletter.\n",
    "* Implement Agile method in lab.\n",
    "* Do CV + NLP course on my own time\n",
    "* Improve technical skills by working\n",
    "* LINXS - events(?)\n",
    "* Log every day\n",
    "* Read 1 paper/ week\n",
    "* Pytorch for projects\n",
    "* NLP pipeline takes importance\n",
    "* Apply for 1 grant at least\n",
    "* Collaborate with Salma. I lead NLP and Salma Image processing. But we help each other out and collaborate.\n",
    "* Aim to publish 8 papers from the lab this year.\n",
    "\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "__\n",
    "\n",
    "### Next Steps\n",
    "Continuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8-10th February 2020\n",
    "---\n",
    "### Project\n",
    "Transcriptomics paper\n",
    "\n",
    "### Aim\n",
    "Readings\n",
    "\n",
    "### Description\n",
    "\n",
    "### Results/Conclusions\n",
    "Had some difficulties understanding the biological parts\n",
    "\n",
    "### Next Steps\n",
    "Continuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15th February 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Building the pipeline\n",
    "\n",
    "### Description\n",
    "Started with the loader scripts of Petter-Alexander. Going to continue further with sentencer and NER.\n",
    "\n",
    "### Results/Conclusions\n",
    "The loader scripts run smoothly without issues on Jupyter. Need to understand how the models of Antton and Petter-Alexander compare in terms of the gold standard corpus\n",
    "\n",
    "### Next Steps\n",
    "Use the sentencer and NER and run predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16th February 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline - Antton\n",
    "\n",
    "### Aim\n",
    "Meeting with Antton about BioBERT, gold standard and differences between the numbers from evaluations (precision/recall)\n",
    "\n",
    "### Description\n",
    "Following points were discussed:\n",
    "\n",
    "1. For Sofi's use evaluator script, each word class should be exclusively defined.\n",
    "2. The gold standard was manually annotated by sonja/salma\n",
    "3. The reason behind the differences in the numbers is due to running different scripts. The bioBERT native evaluator (in perl) is used for the models with good results while sofi's evaluation against the gold standard gives lower numbers. The way Antton used his results for bioBERT is that he replaced the test file with the gold standard one.\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "Antton's percentages are low. Are we going to use his models?\n",
    "\n",
    "### Next Steps\n",
    "Continue with pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19th February 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Data loader fix\n",
    "\n",
    "### Description\n",
    "CORD loader for the pipeline works but the data loader had a bug with small beta unicode character from one of the intended papers. UTF-8 encoding for reading all files still did not work. Turns out setting # -*- coding: utf-8 -*- for linux will work. For me, I needed to reload the notebook after all updates.  \n",
    "\n",
    "### Results/Conclusions\n",
    "Bug fixed. Downloader works on jupyter. For linux utf-8 is default but for windows they are not. So every instance of data loading should be done using utf8. It may help setting the encoding at the beginning of each script (as well as usr/bin/python etc) for future issues.\n",
    "\n",
    "### Next Steps\n",
    "Data loader with abstracts and with full data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22-23th February 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline \n",
    "\n",
    "### Aim\n",
    "Data loader (downloader) with abstract and full text - incomplete.\n",
    "\n",
    "### Description\n",
    "Initial investigation into data loading using utilities as done by petter-alexander allow passing different keywords.\n",
    "https://dataguide.nlm.nih.gov/eutilities/utilities.html.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "\n",
    "\n",
    "### Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23th February 2020\n",
    "---\n",
    "### Project\n",
    "Journal Club\n",
    "\n",
    "### Aim\n",
    "Hovernet paper read and go through competitors. Some algorithms were patchperpix, stardist and CPP-net\n",
    "\n",
    "### Description\n",
    "Investigated a few papers that compare the hovernet paper:\n",
    "\n",
    "1. Dense Steerable Filter CNNs for Exploiting Rotational Symmetry in Histology Images (2020 APR): https://arxiv.org/pdf/2004.03037v2.pdf\n",
    "    The paper compares with hover-net and it seems to have almost similar results to the hovernet\n",
    "    \n",
    "2. An upcoming paper (arxiv) called CPP-Net compares some of these algorithms for segmentation on BBBC006 and data science bowl 2018 datasets. The somparison shows CPP-net to perform best under their evaluation conditions, however hover-net does not perform too badly either.\n",
    "\n",
    "### Additional notes\n",
    "CPP codes are not available since the paper is still on Arxiv. However, the stardist code is.\n",
    "\n",
    "### Results/Conclusions\n",
    "I think it is a paper worth talking about. Sonja suggested we keep an eye out for the CPP-net paper for the future \n",
    "\n",
    "### Next Steps\n",
    "Reading hover-net paper and code and prepare a presentation by 25th Feb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25th February 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Run NER analysis\n",
    "\n",
    "### Description\n",
    "Metrics code was fixed and NER predictions run based on B4CHEMD models. The script too around 30 mins to run and gave the metrics file.\n",
    "\n",
    "### Results/Conclusions\n",
    "Metrics seems to be good. There are multiple precision-recall scores. I am not sure what they stand for.\n",
    "\n",
    "### Next Steps\n",
    "Continue with loader script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd March 2020\n",
    "---\n",
    "### Project\n",
    "Transcriptomics thesis\n",
    "\n",
    "### Aim\n",
    "Note of comments made about the paper to Sonja\n",
    "\n",
    "### Description\n",
    "\"\"\n",
    "I didn't find a lot to comment on the thesis. I looked at the network parts. If I understood correctly, they used the network only to search and look into gene cascades, but they don't do (or show) any analysis with gene interactions. I think a better approach will be to quantify their analysis for their claims.\n",
    "\n",
    "They claim that if some of the neighboring genes are up regulated, they can say that nfkb is activated in the cell. Similarly, they have mentioned a few times that \"some\" genes are found to be important. So I am questioning the following:\n",
    "\n",
    "\t1. How are these genes found?\n",
    "\t3. How did they show that they are important?\n",
    "\t3. Are these genes frequent across multiple networks? Perhaps that can be a stronger claim?\n",
    "\t4. They mention about statistical significance but I don't see how they came up with that.\n",
    "\n",
    "Their conclusion is also weird, they claim that the nfkb is significant, but also say that, perhaps the nfkb regulated inflammation does not play a key role in neuroinflammation. It seems a bit strange to me that they mentioned that.\n",
    "\n",
    "\"\"\n",
    "\n",
    "### Results/Conclusions\n",
    "Seeing an analysis script for the paper may be better.\n",
    "\n",
    "### Next Steps\n",
    "Response from Sonja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd March 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "NER tagger and other models evaluation\n",
    "\n",
    "### Description\n",
    "The B4CHEMD model is actually BC5CDR-Chem model. An adjustment needs to be made for the script.\n",
    "\n",
    "Sonja shared a drive folder with me with the models: https://drive.google.com/file/d/1OletxmPYNkz2ltOr9pyT0b0iBtUWxslh/view\n",
    "These are the files from the BioBERT repo at https://github.com/dmis-lab/biobert and contain all such models.\n",
    "\n",
    "I looked at the training metrics and downloaded the files. They need to be run together with Anttons models\n",
    "\n",
    "The script needs to be adjusted for different models, so that the metrics output file can be run without deletion. Perhaps a table can be created instantly.\n",
    "\n",
    "### Results/Conclusions\n",
    "Continue\n",
    "\n",
    "### Next Steps\n",
    "Make adjustments to the script and run predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd March 2020\n",
    "---\n",
    "### Project\n",
    "AI4MED Journal club compute\n",
    "\n",
    "### Aim\n",
    "Preparations for the first journal club presentation tomorrow\n",
    "\n",
    "### Description\n",
    "1. Read hover-net paper again\n",
    "2. Fix slides accordingly\n",
    "3. Code usage if time permits\n",
    "4. Read cellpose paper\n",
    "5. Read some articles about nuclear segmentation and histology\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "Hopefully it goes well\n",
    "\n",
    "### Next Steps\n",
    "Club meeting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7th April 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Meeting with Sonja and Marcus about Docria\n",
    "\n",
    "### Description\n",
    "Docria is a annotation tool developed by Marcus. The goal is to convert documents to docria annotations. Need to use pmc url to get single documents but there are ways to download documents in bulk.\n",
    "\n",
    "Docria documentation: https://docria.readthedocs.io/en/latest/\n",
    "PMC bioc format instructions: https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/BioC-PMC/\n",
    "\n",
    "Use the following link to access the repo: https://ftp.ncbi.nlm.nih.gov/pub/pmc/\n",
    "\n",
    "### Results/Conclusions\n",
    "Here we try to use bioc to get documents. However, bulk download may not be possible for this. We may need to go back to entrez url formats.\n",
    "\n",
    "### Next Steps\n",
    "Learn and convert one document to docria "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8-9th April 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Docria procedures\n",
    "\n",
    "### Description\n",
    "Docria seems to miss a lot of documentation. \n",
    "\n",
    "Questions\n",
    "\n",
    "1. Couldn't understand the document collections part. Should I convert the file into a MsgpackDocument first? The instructions say that the \"path to your docria file.docria\". Does it mean that we use a docria format file to read and write document collections?\n",
    "\n",
    "2. For layers and field query, several tokens are converted to nodes. What is the point of validate() here?\n",
    "\n",
    "\n",
    "\n",
    "comments\n",
    "1. Maintext function doesn't have a docstring\n",
    "2. Several docstring in document function are missing or very short explanation\n",
    "3. In https://docria.readthedocs.io/en/latest/, for writing document collection, the MsgpackDocumentWriter was not imported.\n",
    "4. Rawdow (instead of rawdoc) speeling error on reading and writing documents section.\n",
    "\n",
    "### Results/Conclusions\n",
    "A bit of experimentation is required but marcus can give a quick breakdown. His github is also more improved than the pypi.\n",
    "\n",
    "### Next Steps\n",
    "__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26th April 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Docria data loader\n",
    "\n",
    "### Description\n",
    "1. Worked on the data loader for docria. \n",
    "\n",
    "Sonja suggested the following:\n",
    "1. make a text collection of the json files of abstracts on this list https://github.com/Aitslab/nlp_2021_alexander_petter/blob/master/pmid.txt.\n",
    "2. use efetch utility for downloading in bulk, but use this as a secondary option\n",
    "3. Bulk download pubmed, pmc, cord-19 and europe pmc subset then add onto docria. We have the following:  \n",
    "    a. PubMed: we need the annual baseline until 2020 and then the subsequent daily update files:  \n",
    "        https://www.nlm.nih.gov/databases/download/pubmed_medline.html  \n",
    "        https://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/   \n",
    "    b. pmc: three different parts: OA subset, author manuscript collection and historical OCR collection all available at: https://www.ncbi.nlm.nih.gov/pmc/tools/textmining/  \n",
    "        OA subset: https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_bulk/  \n",
    "        manuscripts: https://ftp.ncbi.nlm.nih.gov/pub/pmc/manuscript/  \n",
    "        historical PMC: https://ftp.ncbi.nlm.nih.gov/pub/pmc/historical_ocr/  \n",
    "    c. CORD-19 already available  \n",
    "    d. Europe PMC preprint subset: https://europepmc.org/downloads/preprints  \n",
    "        Abstracts: ftp://ftp.ebi.ac.uk/pub/databases/pmc/preprint_abstracts  \n",
    "        Fulltext preprints: ftp://ftp.ebi.ac.uk/pub/databases/pmc/preprints  \n",
    "    \n",
    "4. Note that the data will be in different formats (XML and JSON) with different IDs. Metadata should be set accordingly.\n",
    "    \n",
    "\n",
    "### Results/Conclusions\n",
    "Start with Lunarc Downloads\n",
    "\n",
    "### Next Steps\n",
    "__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28th April 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Docria data loader\n",
    "\n",
    "### Description\n",
    "1. Interesting github repo to work with pdf: https://github.com/kermitt2/grobid\n",
    "\n",
    "2. Found a nice way to download files from an ftp folder:\n",
    "\n",
    "        wget -r -N -l inf ftp://ftp.abcd.com  \n",
    "    \n",
    "\n",
    "3. Put files onto lunarc\n",
    "\n",
    "4. Each article has different format. Extracting text from xml proved to be difficult. However, I was able to find the correct fields for bioc. Continued with docria.\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "Continue\n",
    "\n",
    "### Next Steps\n",
    "Continue with docria and finish up downloads onto lunarc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 29th April 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Docria implementation\n",
    "\n",
    "### Description\n",
    "Lunarc uploads done. \n",
    "\n",
    "Worked with elements of the JSON file. There is no standardized format. It is difficult to extract the information into layers.\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "Continue\n",
    "\n",
    "### Next Steps\n",
    "Continue with docria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10th May 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Docria implementation\n",
    "\n",
    "### Description\n",
    "Working on sections to create a format like the following:\n",
    "\n",
    " \\# - Section name - text\n",
    " 0 - introduction - span + text\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "Continue\n",
    "\n",
    "### Next Steps\n",
    "Continue with docria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12th May 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Docria implementation\n",
    "\n",
    "### Description\n",
    "Build sections layer. Parse files.\n",
    "\n",
    "Building each section required specific approaches. Marcus's solution perhaps works on CORD but not on bioc. Therefore needed to consider each key within the JSON file and parse them into sections. Still it doesn't work for all files. Need to improve upon it. But it is not urgent at the moment.\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "Continue\n",
    "\n",
    "### Next Steps\n",
    "Continue with docria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13th May 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Docria implementation\n",
    "\n",
    "### Description\n",
    "Built reference layer on Docria. Collecting all citations and converting them into nodes. Now we have\n",
    "``` \n",
    "[bib_refs]\n",
    " * pmid    : str[default=]\n",
    " * title   : str[default=]\n",
    " * authors : str[default=]\n",
    " * year    : i32[default=0]\n",
    " * source  : str[default=]\n",
    " * pages   : str[default=]\n",
    "\n",
    "[sections]\n",
    " * section : str[default=]\n",
    " * text    : span[context=main]\n",
    "```\n",
    "\n",
    "Also arranged meetings with Ola-adam and oskar about using the clusters.\n",
    "\n",
    "### Results/Conclusions\n",
    "Continue\n",
    "\n",
    "### Next Steps\n",
    "Continue with docria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14th May 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Docria implementation\n",
    "\n",
    "### Description\n",
    "Do minor adjustments in docria. Add ID layer for ease of reference.\n",
    "``` \n",
    " [IDs]\n",
    " * ID_type : str[default=]\n",
    " * ID      : str[default=]\n",
    "\n",
    "[bib_refs]\n",
    " * pmid    : str[default=]\n",
    " * title   : str[default=]\n",
    " * authors : str[default=]\n",
    " * year    : i32[default=0]\n",
    " * source  : str[default=]\n",
    " * pages   : str[default=]\n",
    "\n",
    "[sections]\n",
    " * section : str[default=]\n",
    " * text    : span[context=main]\n",
    "```\n",
    "\n",
    "Added a repolist folder on lunarc to locally save some repos.\n",
    "\n",
    "Sonja additionally gave some information:\n",
    "\n",
    "1. we will soon have a first minimal version of our NLP pipeline. Petter and Alexander have written scripts that can use a txt file with pmids as input, download abstracts, split in sentences and predict chemical entities with bioBERT. I have written a script that can add brackets to those sentences with two entities. and Lykke and Klara have added to Petter's and Alexander's script and trained a relation model so that we have a final module that returns relations from sentences that had two entities in a tsv file. the file can then be imported into cytoscape to visualise the relations.\n",
    "\n",
    "2. Add NER and RE models (chem ++) then we also want to add the extracted info as layers into docria.\n",
    "\n",
    "3. Soon we will have a working pipeline.\n",
    "\n",
    "4. \n",
    "\n",
    "### Results/Conclusions\n",
    "Continue\n",
    "\n",
    "### Next Steps\n",
    "Continue with docria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17th May 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Various\n",
    "\n",
    "### Description\n",
    "1. Had a meeting with Adam/Ola\n",
    "2. Looked at sonja's artificial corpus generator\n",
    "3. Continued working with docria wwith cord dataset\n",
    "\n",
    "### Results/Conclusions\n",
    "Continue\n",
    "\n",
    "### Next Steps\n",
    "Continue with docria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19th May 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Docria Implementation\n",
    "\n",
    "### Description\n",
    "Was able to figure out how to get different sections for CORD data. Note that not all information was available for cord data. \n",
    "\n",
    "Also, the cord dataset is split into two different groups: pmc_json and pdf_json. Pmc_json has the real JSON format full articles where pdf_json has pdf articles converted to json by using grobid. All in all, these are not entirely reliable. \n",
    "\n",
    "### Results/Conclusions\n",
    "I will work with pmc_json folder for now.  Will try pdf_json at a later time.\n",
    "\n",
    "### Next Steps\n",
    "Continue with docria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20-21th May 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Docria Implementation + more\n",
    "\n",
    "### Description\n",
    "Created a collection of cord abstracts for docria. It contains the main text (content), title, IDs.\n",
    "\n",
    "### Results/Conclusions\n",
    "The collection looks fine\n",
    "\n",
    "### Next Steps\n",
    "Continue with docria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20-21th May 2020\n",
    "---\n",
    "### Project\n",
    "NLP SWE\n",
    "\n",
    "### Aim\n",
    "Pattern matching for Johanna's patient data\n",
    "\n",
    "### Description\n",
    "Created a notebook using tika API to load and parse information from patient data for Johanna. It worked pretty well. It was able to extract the primary information she asked for, the field info if passed and also allowed hardcoded functions to extract information like temperature and such.\n",
    "\n",
    "### Results/Conclusions\n",
    "It worked and she found it useful\n",
    "\n",
    "### Next Steps\n",
    "__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24th May 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Try running nicholas's code on cluster\n",
    "\n",
    "### Description\n",
    "Nicholas uses the neuralcoref pipeline run on the Craft corpora. His code is available here: https://github.com/Aitslab/BioNLP/tree/master/nicolas\n",
    "\n",
    "Whereas his train/test files (train subset1) are here:\n",
    "https://drive.google.com/drive/folders/1JVYIHJU-HWe1RAfkHKAVUZvBWOenKtUg\n",
    "\n",
    "I created a folder in alvis and tried running his script but it didn't work. I asked Salma to help me with it. She was eventually able to run it.\n",
    "\n",
    "### Results/Conclusions\n",
    "I faced several issues with the script. I need to double check the actual code before implementing it.\n",
    "\n",
    "### Next Steps\n",
    "Continue with docria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25th May 2020\n",
    "---\n",
    "### Project\n",
    "NLP SWE\n",
    "\n",
    "### Aim\n",
    "Create a simple pipeline for spacy\n",
    "\n",
    "### Description\n",
    "created a simple phrasematcher for spacy, where NER and tokenization is done and visualized with displacy for Kalle to use.\n",
    "\n",
    "### Results/Conclusions\n",
    "It worked and kalle used it to improve his scripts\n",
    "\n",
    "### Next Steps\n",
    "__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26th May 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Try running nicholas's code on cluster\n",
    "\n",
    "### Description\n",
    "Running nicholas's script on cluster did not work. I faced issues with scoring part within the perl script. However, salma tweeked the code in a way that it ran.\n",
    "\n",
    "### Results/Conclusions\n",
    "The attempt was unsuccessful.\n",
    "\n",
    "### Next Steps\n",
    "Continue with docria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27th May 2020\n",
    "---\n",
    "### Project\n",
    "Ancient DNA\n",
    "\n",
    "### Aim\n",
    "Initial meeting and tasks\n",
    "\n",
    "### Description\n",
    "Had a meeting with potential collaborators about ancient DNA. The project, if successful, will be a substitute for carbon dating.\n",
    "\n",
    "I have the following tasks:\n",
    "1. use the data to build a random forest classifier\n",
    "2. use the data to see if a knn classifier works\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "Sonja suggested not to spend too much time on it \n",
    "I wrote down my questions here: https://docs.google.com/document/d/1pbg8DwZsXhMQIBeMXWaWLlWA6-tovRw7xZE73qX1AQg/edit?usp=sharing\n",
    "\n",
    "### Next Steps\n",
    "Implement the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31st May 2020\n",
    "---\n",
    "### Project\n",
    "Journal club\n",
    "\n",
    "### Aim\n",
    "discussion about the journal club.\n",
    "\n",
    "### Description\n",
    "topic NLP: few shot learning\n",
    "\n",
    "example: https://arxiv.org/abs/2012.14978\n",
    "\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "no particular date set so far.\n",
    "\n",
    "### Next Steps\n",
    "__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-11 June 2020\n",
    "---\n",
    "### Course: Oral Communication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14th June 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Continuation with docria\n",
    "\n",
    "### Description\n",
    "Looking at spans for sentences and applying petter-alexander script for ner. It works but couldn't figure out spans for sentences.\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "Confused about how to fix span issue\n",
    "\n",
    "\n",
    "### Next Steps\n",
    "Continue working and fix span issue for sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15th June 2020\n",
    "---\n",
    "### Project\n",
    "NLP Pipeline\n",
    "\n",
    "### Aim\n",
    "Looked at pb to onnx conversion using tf2onnx.\n",
    "\n",
    "### Description\n",
    "I faced the following problems while using the tf2onnx conversion:\n",
    "\n",
    "1. tensorflow issue: the model could not be recognized. Used tf.compat.v1 as tf to solve this\n",
    "\n",
    "2. path issue: model was not being recognized within the path in the script. This is unusual since python doesn't need full path. Used full path to solve this.\n",
    "\n",
    "3. The last error I get is \"str object has no attribute graph\". Couldn't solve this one yet.\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "Need to solve issue #3\n",
    "\n",
    "\n",
    "### Next Steps\n",
    "Run tf2onnx properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18th June 2020\n",
    "---\n",
    "### Project\n",
    "Ancient DNA\n",
    "\n",
    "### Aim\n",
    "Data exploration and random forest\n",
    "\n",
    "### Description\n",
    "Checked the data from Eran and did a brief analysis step. Followed that by applying a scikit-learn random forest classifier and a regressor (the correct approach) on the data. The data was merged and split into 80-20 train test split. \n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "The results had around 90% train and 64% test accuracy. Which is not the best. Need to do a finetuning of the data.\n",
    "\n",
    "\n",
    "### Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21th June 2020\n",
    "---\n",
    "### Project\n",
    "Ancient DNA\n",
    "\n",
    "### Aim\n",
    "Data exploration, random forest, finetuning and XGboost\n",
    "\n",
    "### Description\n",
    "Continued the past approach and tried finetuning the tree parameters. Turns out the max accuracy I could get was 66% - close to the previous results.\n",
    "\n",
    "Followed the random forest approach with an XGboost regressor. The final result was similar to the random forest approach, but much lower.\n",
    "A repo was created at: https://github.com/rafsanahmed/ancient_dna\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "The initial results seem not that promising. However, I still need to try transfer learning and cross validation.\n",
    "\n",
    "\n",
    "### Next Steps\n",
    "Cross validation and perhaps NN and transfer learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28th June 2020\n",
    "---\n",
    "### Project\n",
    "Staff appraisal meeting\n",
    "\n",
    "### Aim\n",
    "Evaluate, set goals, discuss tasks etc. \n",
    "\n",
    "### Description\n",
    "The following points were discussed: \n",
    "\n",
    "1.\n",
    "\n",
    "\n",
    "### Results/Conclusions\n",
    "\n",
    "\n",
    "\n",
    "### Next Steps\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
